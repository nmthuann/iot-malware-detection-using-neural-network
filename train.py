import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import os
import time

#cái này để chỉnh số lượng thread tham gia vào tính toán -> càng cao càng tốn cpu
tf.config.threading.set_inter_op_parallelism_threads(2)
tf.config.threading.set_intra_op_parallelism_threads(2)

img_height = 224
img_width = 224
batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
    'out', labels='inferred', 
    label_mode='int',
    color_mode='rgb', 
    batch_size=batch_size, 
    image_size=(img_height,img_width), 
    shuffle=False, 
    validation_split=0.1, 
    subset='training',
    interpolation='bilinear'
)

validation_ds = tf.keras.utils.image_dataset_from_directory(
    'out', labels='inferred', 
    label_mode='int',
    color_mode='rgb', 
    batch_size=batch_size, 
    image_size=(img_height,img_width), 
    shuffle=False, 
    validation_split=0.1, 
    subset='validation',
    interpolation='bilinear'
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    'test', labels='inferred', 
    label_mode='int',
    color_mode='rgb', 
    batch_size=batch_size, 
    image_size=(img_height,img_width), 
    shuffle=False,
    interpolation='bilinear'
)

train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()
test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()
validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()
print("Training data size:", train_ds_size)
print("Test data size:", test_ds_size)
print("Validation data size:", validation_ds_size)

#phải bỏ shuffle đi vì shuffle dùng RAM để lưu dataset. nhưng do dataset lớn quá -> tràn RAM
# train_ds = (train_ds
#                   .map(process_images)
#                 #   .shuffle(buffer_size=train_ds_size)
#                   .batch(batch_size=32, drop_remainder=True))
# test_ds = (test_ds
#                   .map(process_images)
#                 #   .shuffle(buffer_size=train_ds_size)
#                   .batch(batch_size=32, drop_remainder=True))
# validation_ds = (validation_ds
#                   .map(process_images)
#                 #   .shuffle(buffer_size=train_ds_size)
#                   .batch(batch_size=32, drop_remainder=True))

#OLD AlexNet
# with tf.device('/CPU:0'):
    # model = keras.models.Sequential([
    #     keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(224,224,3)),
    #     keras.layers.BatchNormalization(),
    #     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    #     keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding="same"),
    #     keras.layers.BatchNormalization(),
    #     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    #     keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    #     keras.layers.BatchNormalization(),
    #     keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    #     keras.layers.BatchNormalization(),
    #     keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    #     keras.layers.BatchNormalization(),
    #     keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    #     keras.layers.Flatten(),
    #     keras.layers.Dense(4096, activation='relu'),
    #     keras.layers.Dropout(0.5),
    #     keras.layers.Dense(4096, activation='relu'),
    #     keras.layers.Dropout(0.5),
    #     keras.layers.Dense(10, activation='softmax')
    # ])

model = keras.models.Sequential()
model.add(layers.Conv2D(96, 11, strides=4, padding='same', input_shape=(224,224,3)))
model.add(layers.Lambda(tf.nn.local_response_normalization))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(3, strides=2))
model.add(layers.Conv2D(256, 5, strides=1, padding='same'))
model.add(layers.Lambda(tf.nn.local_response_normalization))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(3, strides=2))
model.add(layers.Conv2D(384, 3, strides=1, padding='same'))
model.add(layers.Activation('relu'))
model.add(layers.Conv2D(384, 3, strides=1, padding='same'))
model.add(layers.Activation('relu'))
model.add(layers.Conv2D(256, 3, strides=1, padding='same'))
model.add(layers.Activation('relu'))
model.add(layers.Flatten())
model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(2, activation='softmax'))
model.summary()

root_logdir = os.path.join(os.curdir, "logs\\fit\\")

def get_run_logdir():
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)

model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(learning_rate=0.001), metrics=['accuracy'])
model.summary()

checkpoint_path = "training/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
# Create a callback that saves the model's weights every 5 epochs

cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path, 
    verbose=1, 
    save_weights_only=True,
    save_freq=5*batch_size)

model.fit(train_ds,
            epochs=10,
            validation_data=validation_ds,
            validation_freq=1,
            callbacks=[tensorboard_cb])

evaluation = model.evaluate(test_ds, return_dict=True)
print()

for name, value in evaluation.items():
    print(f"{name}: {value:.4f}")

model.save('saved_model/model_1')