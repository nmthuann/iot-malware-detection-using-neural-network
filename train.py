import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import os
import time

tf.config.threading.set_inter_op_parallelism_threads(2)
tf.config.threading.set_intra_op_parallelism_threads(2)

img_height = 224
img_width = 224
batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
    'out', labels='inferred', 
    label_mode='int',
    color_mode='rgb', 
    batch_size=batch_size, 
    image_size=(img_height,img_width), 
    shuffle=False, 
    validation_split=0.1, 
    subset='training',
    interpolation='bilinear'
)

validation_ds = tf.keras.utils.image_dataset_from_directory(
    'out', labels='inferred', 
    label_mode='int',
    color_mode='rgb', 
    batch_size=batch_size, 
    image_size=(img_height,img_width), 
    shuffle=False, 
    validation_split=0.1, 
    subset='validation',
    interpolation='bilinear'
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    'test', labels='inferred', 
    label_mode='int',
    color_mode='rgb', 
    batch_size=batch_size, 
    image_size=(img_height,img_width), 
    shuffle=False,
    interpolation='bilinear'
)

train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()
test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()
validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()
print("Training data size:", train_ds_size)
print("Test data size:", test_ds_size)
print("Validation data size:", validation_ds_size)

model = keras.models.Sequential()
model.add(layers.Conv2D(96, 11, strides=4, padding='same', input_shape=(224,224,3)))
model.add(layers.Lambda(tf.nn.local_response_normalization))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(3, strides=2))
model.add(layers.Conv2D(256, 5, strides=1, padding='same'))
model.add(layers.Lambda(tf.nn.local_response_normalization))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(3, strides=2))
model.add(layers.Conv2D(384, 3, strides=1, padding='same'))
model.add(layers.Activation('relu'))
model.add(layers.Conv2D(384, 3, strides=1, padding='same'))
model.add(layers.Activation('relu'))
model.add(layers.Conv2D(256, 3, strides=1, padding='same'))
model.add(layers.Activation('relu'))
model.add(layers.Flatten())
model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(2, activation='softmax'))
model.summary()

root_logdir = os.path.join(os.curdir, "logs\\fit\\")

def get_run_logdir():
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)

model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(learning_rate=0.001), metrics=['accuracy'])
model.summary()

# Save weight per epoch callback
# checkpoint_path = "training/cp-{epoch:04d}.ckpt"
# checkpoint_dir = os.path.dirname(checkpoint_path)
# # Create a callback that saves the model's weights every 5 epochs

# cp_callback = tf.keras.callbacks.ModelCheckpoint(
#     filepath=checkpoint_path, 
#     verbose=1, 
#     save_weights_only=True,
#     save_freq=5*batch_size)

model.fit(train_ds,
            epochs=10,
            validation_data=validation_ds,
            validation_freq=1,
            callbacks=[tensorboard_cb])

evaluation = model.evaluate(test_ds, return_dict=True)

print("[+] Result:")

for name, value in evaluation.items():
    print(f"{name}: {value:.4f}")

model.save_weights('./chk_points/checkpoint_1')